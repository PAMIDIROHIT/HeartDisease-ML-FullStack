{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bacc81c",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Model Training\n",
    "\n",
    "This notebook demonstrates the training process for various machine learning models to predict heart disease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ef06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    # Try to load from the data directory\n",
    "    df = pd.read_csv('../data/raw/heart.csv')\n",
    "except FileNotFoundError:\n",
    "    # If not found, try the project root\n",
    "    try:\n",
    "        df = pd.read_csv('../../heart.csv')\n",
    "    except FileNotFoundError:\n",
    "        # Create sample data for demonstration\n",
    "        print(\"Dataset not found. Creating sample data for demonstration.\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        df = pd.DataFrame({\n",
    "            'age': np.random.randint(25, 80, n_samples),\n",
    "            'sex': np.random.choice([0, 1], n_samples),\n",
    "            'cp': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "            'trestbps': np.random.randint(90, 200, n_samples),\n",
    "            'chol': np.random.randint(120, 400, n_samples),\n",
    "            'fbs': np.random.choice([0, 1], n_samples),\n",
    "            'restecg': np.random.choice([0, 1, 2], n_samples),\n",
    "            'thalach': np.random.randint(70, 200, n_samples),\n",
    "            'exang': np.random.choice([0, 1], n_samples),\n",
    "            'oldpeak': np.random.uniform(0, 6, n_samples),\n",
    "            'slope': np.random.choice([0, 1, 2], n_samples),\n",
    "            'ca': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "            'thal': np.random.choice([0, 1, 2], n_samples),\n",
    "            'target': np.random.choice([0, 1], n_samples)\n",
    "        })\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5812b2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to train\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'XGBoost': XGBClassifier(random_state=42),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "print(\"Training and evaluating models...\")\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Use scaled data for models that benefit from it\n",
    "    if name in ['Logistic Regression', 'SVM', 'Neural Network']:\n",
    "        X_train_model = X_train_scaled\n",
    "        X_test_model = X_test_scaled\n",
    "    else:\n",
    "        X_train_model = X_train\n",
    "        X_test_model = X_test\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_model)\n",
    "    y_pred_proba = model.predict_proba(X_test_model)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Cross-validation score\n",
    "    cv_scores = cross_val_score(model, X_train_model, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"  CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    name: {\n",
    "        'Accuracy': results['accuracy'],\n",
    "        'Precision': results['precision'],\n",
    "        'Recall': results['recall'],\n",
    "        'F1 Score': results['f1_score'],\n",
    "        'ROC AUC': results['roc_auc'],\n",
    "        'CV Accuracy': results['cv_mean']\n",
    "    }\n",
    "    for name, results in model_results.items()\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.T\n",
    "comparison_df = comparison_df.sort_values('F1 Score', ascending=False)\n",
    "\n",
    "print(\"Model Comparison (sorted by F1 Score):\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "comparison_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']].plot(kind='bar', ax=ax)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e48d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1 score\n",
    "best_model_name = comparison_df.index[0]\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_model_results = model_results[best_model_name]\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"F1 Score: {best_model_results['f1_score']:.4f}\")\n",
    "print(f\"ROC AUC: {best_model_results['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ebb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_model_results['y_pred'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report - {best_model_name}:\\n\")\n",
    "print(classification_report(y_test, best_model_results['y_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cfe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866025b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for top models\n",
    "print(\"Performing hyperparameter tuning for top models...\")\n",
    "\n",
    "# Select top 3 models for hyperparameter tuning\n",
    "top_models = comparison_df.head(3).index.tolist()\n",
    "print(f\"Top 3 models for tuning: {top_models}\")\n",
    "\n",
    "# Define parameter grids\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2]\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'penalty': ['l1', 'l2']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "tuned_results = {}\n",
    "\n",
    "for model_name in top_models:\n",
    "    if model_name in param_grids:\n",
    "        print(f\"\\nTuning {model_name}...\")\n",
    "        \n",
    "        # Select appropriate model and data\n",
    "        if model_name == 'Logistic Regression':\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            X_train_tune = X_train_scaled\n",
    "            X_test_tune = X_test_scaled\n",
    "        elif model_name == 'XGBoost':\n",
    "            model = XGBClassifier(random_state=42)\n",
    "            X_train_tune = X_train\n",
    "            X_test_tune = X_test\n",
    "        else:  # Random Forest\n",
    "            model = RandomForestClassifier(random_state=42)\n",
    "            X_train_tune = X_train\n",
    "            X_test_tune = X_test\n",
    "        \n",
    "        # Perform grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            model, \n",
    "            param_grids[model_name], \n",
    "            cv=5, \n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_tune, y_train)\n",
    "        \n",
    "        # Evaluate tuned model\n",
    "        best_tuned_model = grid_search.best_estimator_\n",
    "        y_pred_tuned = best_tuned_model.predict(X_test_tune)\n",
    "        y_pred_proba_tuned = best_tuned_model.predict_proba(X_test_tune)[:, 1]\n",
    "        \n",
    "        tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "        tuned_f1 = f1_score(y_test, y_pred_tuned)\n",
    "        tuned_roc_auc = roc_auc_score(y_test, y_pred_proba_tuned)\n",
    "        \n",
    "        tuned_results[model_name] = {\n",
    "            'model': best_tuned_model,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'accuracy': tuned_accuracy,\n",
    "            'f1_score': tuned_f1,\n",
    "            'roc_auc': tuned_roc_auc\n",
    "        }\n",
    "        \n",
    "        print(f\"  Best Parameters: {grid_search.best_params_}\")\n",
    "        print(f\"  F1 Score: {tuned_f1:.4f} (improved from {model_results[model_name]['f1_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc135bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs tuned models\n",
    "if tuned_results:\n",
    "    comparison_data = []\n",
    "    for model_name in tuned_results.keys():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': 'Original',\n",
    "            'F1 Score': model_results[model_name]['f1_score'],\n",
    "            'ROC AUC': model_results[model_name]['roc_auc']\n",
    "        })\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Type': 'Tuned',\n",
    "            'F1 Score': tuned_results[model_name]['f1_score'],\n",
    "            'ROC AUC': tuned_results[model_name]['roc_auc']\n",
    "        })\n",
    "    \n",
    "    comparison_tuned_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    comparison_tuned_df.set_index(['Model', 'Type'])['F1 Score'].unstack().plot(kind='bar', ax=ax)\n",
    "    plt.title('Original vs Tuned Model Performance')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Type')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Performance Comparison (Original vs Tuned):\")\n",
    "    print(comparison_tuned_df.pivot(index='Model', columns='Type', values=['F1 Score', 'ROC AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf373d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select final best model\n",
    "final_model = None\n",
    "final_model_name = \"\"\n",
    "final_f1_score = 0\n",
    "\n",
    "# Check tuned models\n",
    "for model_name, results in tuned_results.items():\n",
    "    if results['f1_score'] > final_f1_score:\n",
    "        final_model = results['model']\n",
    "        final_model_name = f\"{model_name} (Tuned)\"\n",
    "        final_f1_score = results['f1_score']\n",
    "\n",
    "# Check original models if tuned models aren't better\n",
    "for model_name, results in model_results.items():\n",
    "        if results['f1_score'] > final_f1_score:\n",
    "            final_model = results['model']\n",
    "            final_model_name = model_name\n",
    "            final_f1_score = results['f1_score']\n",
    "\n",
    "print(f\"\\nFinal Best Model: {final_model_name}\")\n",
    "print(f\"Final F1 Score: {final_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f864530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and scaler\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = '../models/trained_models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(models_dir, 'best_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(final_model, f)\n",
    "\n",
    "# Save the scaler\n",
    "scaler_path = os.path.join(models_dir, 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "features_path = os.path.join(models_dir, 'feature_names.pkl')\n",
    "with open(features_path, 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "print(f\"Feature names saved to: {features_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aff24a",
   "metadata": {},
   "source": [
    "## Model Training Summary\n",
    "\n",
    "### Models Trained:\n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "4. Support Vector Machine\n",
    "5. XGBoost\n",
    "6. Neural Network\n",
    "\n",
    "### Best Performing Model:\n",
    "**{final_model_name}** with F1 Score: {final_f1_score:.4f}\n",
    "\n",
    "### Key Metrics:\n",
    "- **Accuracy**: [Value]\n",
    "- **Precision**: [Value]\n",
    "- **Recall**: [Value]\n",
    "- **F1 Score**: {final_f1_score:.4f}\n",
    "- **ROC AUC**: [Value]\n",
    "\n",
    "### Model Saved:\n",
    "The best model has been saved to `../models/trained_models/best_model.pkl` along with the scaler and feature names.\n",
    "\n",
    "### Next Steps:\n",
    "1. Model evaluation and interpretation\n",
    "2. Feature importance analysis\n",
    "3. Integration with the web application\n",
    "4. Deployment preparation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
