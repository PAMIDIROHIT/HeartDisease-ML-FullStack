{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778591fc",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the trained heart disease prediction model using various metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfeffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, classification_report, \n",
    "    roc_curve, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Loading trained model and test data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    # Try to load from the data directory\n",
    "    df = pd.read_csv('../data/raw/heart.csv')\n",
    "except FileNotFoundError:\n",
    "    # If not found, try the project root\n",
    "    try:\n",
    "        df = pd.read_csv('../../heart.csv')\n",
    "    except FileNotFoundError:\n",
    "        # Create sample data for demonstration\n",
    "        print(\"Dataset not found. Creating sample data for demonstration.\")\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        df = pd.DataFrame({\n",
    "            'age': np.random.randint(25, 80, n_samples),\n",
    "            'sex': np.random.choice([0, 1], n_samples),\n",
    "            'cp': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "            'trestbps': np.random.randint(90, 200, n_samples),\n",
    "            'chol': np.random.randint(120, 400, n_samples),\n",
    "            'fbs': np.random.choice([0, 1], n_samples),\n",
    "            'restecg': np.random.choice([0, 1, 2], n_samples),\n",
    "            'thalach': np.random.randint(70, 200, n_samples),\n",
    "            'exang': np.random.choice([0, 1], n_samples),\n",
    "            'oldpeak': np.random.uniform(0, 6, n_samples),\n",
    "            'slope': np.random.choice([0, 1, 2], n_samples),\n",
    "            'ca': np.random.choice([0, 1, 2, 3], n_samples),\n",
    "            'thal': np.random.choice([0, 1, 2], n_samples),\n",
    "            'target': np.random.choice([0, 1], n_samples)\n",
    "        })\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# For demonstration, we'll split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398c6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and scaler\n",
    "try:\n",
    "    # Try to load from models directory\n",
    "    with open('../models/trained_models/best_model.pkl', 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    with open('../models/trained_models/scaler.pkl', 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "        \n",
    "    print(\"Model and scaler loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    # If files not found, create a simple model for demonstration\n",
    "    print(\"Trained model not found. Creating a simple model for demonstration.\")\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    print(\"Demonstration model created and trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Predictions made successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb779720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"Model Evaluation Metrics:\")\n",
    "print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall: {recall:.4f}\")\n",
    "print(f\"  F1 Score: {f1:.4f}\")\n",
    "print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"  Average Precision: {avg_precision:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24798eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Calculate metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Values:\")\n",
    "print(f\"  True Negatives: {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives: {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a83fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c62cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1eaa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall_vals, precision_vals, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17231b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (if available)\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title('Top 10 Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "else:\n",
    "    print(\"Model does not support feature importance analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225ee627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction probability distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram for each class\n",
    "plt.hist(y_pred_proba[y_test == 0], bins=30, alpha=0.7, label='No Heart Disease', color='blue')\n",
    "plt.hist(y_pred_proba[y_test == 1], bins=30, alpha=0.7, label='Heart Disease', color='red')\n",
    "\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Prediction Probabilities')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d3ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration plot\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"s-\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "plt.xlabel(\"Mean Predicted Probability\")\n",
    "plt.ylabel(\"Fraction of Positives\")\n",
    "plt.title(\"Calibration Plot\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c5043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance by age groups\n",
    "df_test = X_test.copy()\n",
    "df_test['target'] = y_test\n",
    "df_test['predicted'] = y_pred\n",
    "df_test['probability'] = y_pred_proba\n",
    "\n",
    "# Create age groups\n",
    "df_test['age_group'] = pd.cut(df_test['age'], bins=[0, 40, 50, 60, 100], labels=['<40', '40-50', '50-60', '60+'])\n",
    "\n",
    "# Calculate performance by age group\n",
    "performance_by_age = df_test.groupby('age_group').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'accuracy': accuracy_score(x['target'], x['predicted']),\n",
    "        'precision': precision_score(x['target'], x['predicted'], zero_division=0),\n",
    "        'recall': recall_score(x['target'], x['predicted'], zero_division=0),\n",
    "        'f1': f1_score(x['target'], x['predicted'], zero_division=0),\n",
    "        'count': len(x)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"Model Performance by Age Group:\")\n",
    "print(performance_by_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of performance by age group\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "titles = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    axes[i].bar(performance_by_age.index, performance_by_age[metric], color='skyblue')\n",
    "    axes[i].set_title(f'{title} by Age Group')\n",
    "    axes[i].set_xlabel('Age Group')\n",
    "    axes[i].set_ylabel(title)\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for j, v in enumerate(performance_by_age[metric]):\n",
    "        axes[i].text(j, v + 0.02, f'{v:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a43b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis\n",
    "df_test['error'] = (y_test != y_pred).astype(int)\n",
    "\n",
    "# Analyze errors by feature distributions\n",
    "error_analysis = df_test.groupby('error').agg({\n",
    "    'age': ['mean', 'std'],\n",
    "    'trestbps': ['mean', 'std'],\n",
    "    'chol': ['mean', 'std'],\n",
    "    'thalach': ['mean', 'std'],\n",
    "    'oldpeak': ['mean', 'std']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Error Analysis - Feature Statistics by Correct/Incorrect Predictions:\")\n",
    "print(error_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa535aeb",
   "metadata": {},
   "source": [
    "## Model Evaluation Summary\n",
    "\n",
    "### Key Performance Metrics:\n",
    "- **Accuracy**: {accuracy:.4f}\n",
    "- **Precision**: {precision:.4f}\n",
    "- **Recall**: {recall:.4f}\n",
    "- **F1 Score**: {f1:.4f}\n",
    "- **ROC AUC**: {roc_auc:.4f}\n",
    "- **Average Precision**: {avg_precision:.4f}\n",
    "\n",
    "### Model Insights:\n",
    "1. **Overall Performance**: The model shows [good/moderate/poor] performance with an F1 score of {f1:.4f}.\n",
    "2. **Class Balance**: The model [handles/struggles with] the class imbalance in the dataset.\n",
    "3. **Calibration**: The model is [well-calibrated/poorly-calibrated] based on the calibration plot.\n",
    "4. **Feature Importance**: [Most/Less] important features were [feature names].\n",
    "5. **Age Group Performance**: The model performs [consistently/variably] across different age groups.\n",
    "\n",
    "### Recommendations:\n",
    "1. [Recommendation 1]\n",
    "2. [Recommendation 2]\n",
    "3. [Recommendation 3]\n",
    "\n",
    "### Next Steps:\n",
    "1. Model deployment and integration\n",
    "2. Monitoring and maintenance\n",
    "3. Continuous improvement\n",
    "4. User feedback collection"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
